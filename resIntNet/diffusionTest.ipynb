{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e3mZj_0cHyCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fdbc986-789a-43f2-e2f4-f4b3a2b5e20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nkfFUkjUHu65"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data as D\n",
        "import os, re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bitnet\n"
      ],
      "metadata": {
        "id": "op1m877rMqT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchcfm\n",
        "from torchcfm.conditional_flow_matching import *\n",
        "from torchcfm.utils import plot_trajectories, torch_wrapper"
      ],
      "metadata": {
        "id": "G1Fzu4mW0Lc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pdfn0oUHu66"
      },
      "source": [
        "# Data loading\n",
        "- Need to implement way to load using DataLoader class of pytorch.\n",
        "- Create way to create offset tensor set for \"groud truth\" embeddings to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hCDMsLHdHu66"
      },
      "outputs": [],
      "source": [
        "class SimData(D.Dataset):\n",
        "    def __init__(self, file_paths):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            file_paths (list of str): List of paths to the .pt files.\n",
        "        \"\"\"\n",
        "        self.file_paths = file_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the tensor stored at the idx-th path\n",
        "        # Check if file exists\n",
        "        file_path = self.file_paths[idx]\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"No such file or directory: '{file_path}'\")\n",
        "\n",
        "        data = torch.load(file_path)\n",
        "        # Ensure the data has the expected dimensions\n",
        "        if data.size(0) < 2:\n",
        "            raise ValueError(f\"Data at index {idx} does not have the expected dimensions.\")\n",
        "\n",
        "        # Select the data and stepAhead based on the description\n",
        "        data_current = data[:-1]  # All but the last time step\n",
        "        stepAhead = data[1:]      # All but the first time step\n",
        "        return data_current, stepAhead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Kt2yyQ9UHu67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2e8439-280d-4a03-ce06-29b8e5c0ab41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "folder_path = \"/content/drive/MyDrive/ProteinBindingProject/encodedData\" #@param\n",
        "# Check if the directory exists\n",
        "if not os.path.isdir(folder_path):\n",
        "    raise FileNotFoundError(f\"Directory '{folder_path}' does not exist.\")\n",
        "# Get the list of .pt files sorted by the number present at the beginning of the file name\n",
        "pt_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.pt')],\n",
        "    key=lambda x: float(re.match(r'batch_(\\d+)_', x).group(1)))\n",
        "file_paths = []\n",
        "for pt_file in pt_files:\n",
        "    file_paths.append(os.path.join(folder_path, pt_file))\n",
        "\n",
        "for file_path in file_paths:\n",
        "    print(os.path.exists(file_path))\n",
        "\n",
        "# Create dataset\n",
        "dataset = SimData(file_paths)\n",
        "\n",
        "# Create DataLoader\n",
        "data_loader = D.DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcKGCylCHu67"
      },
      "source": [
        "# Diffusion Module from open source implementation of Alphafold3\n",
        "\n",
        "Below are diffusion modules to test for inference of next time point using diffusion, once this is achieved next step will be to implent KAN as predicitve network to create a more tracatable predictive module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgzR5SiRHu67",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Initial Sucess Diffusion steps set at 100 for a model to fit in TPU system RAM size\n",
        "#@markdown - Need to finalize padding size\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "def context_size():\n",
        "  x = 1320 #@param\n",
        "  return x\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_groups):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = int(num_heads)  # Ensure num_heads is an integer\n",
        "        self.num_groups = num_groups\n",
        "        self.head_dim = embed_dim // self.num_heads  # Ensure head_dim is an integer\n",
        "\n",
        "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "        assert embed_dim == self.embed_dim, \"Input embedding dimension must match model embedding dimension\"\n",
        "\n",
        "        qkv = self.qkv_proj(x)\n",
        "        qkv = qkv.view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_len, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        assert seq_len % self.num_groups == 0, \"seq_len must be divisible by num_groups\"\n",
        "        group_size = seq_len // self.num_groups\n",
        "\n",
        "        # Group queries\n",
        "        q_groups = q.view(batch_size, self.num_heads, self.num_groups, group_size, self.head_dim)  # (batch_size, num_heads, num_groups, group_size, head_dim)\n",
        "        k_groups = k.view(batch_size, self.num_heads, self.num_groups, group_size, self.head_dim)\n",
        "        v_groups = v.view(batch_size, self.num_heads, self.num_groups, group_size, self.head_dim)\n",
        "\n",
        "        # Compute attention for each group\n",
        "        attn_scores = torch.einsum('bhgqd,bhgkd->bhgqk', q_groups, k_groups)  # (batch_size, num_heads, num_groups, group_size, group_size)\n",
        "        attn_scores = attn_scores / (self.head_dim ** 0.5)\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        attn_output = torch.einsum('bhgqk,bhgvd->bhgqd', attn_probs, v_groups)  # (batch_size, num_heads, num_groups, group_size, head_dim)\n",
        "        attn_output = attn_output.contiguous().view(batch_size, self.num_heads, seq_len, self.head_dim)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        return self.out_proj(attn_output)\n",
        "\n",
        "def modulate(normed_x, shift, scale):\n",
        "    return normed_x * (1 + scale) + shift\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_groups, feedforward_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim, elementwise_affine=False, eps=1e-6)\n",
        "        self.attention = GroupedQueryAttention(embed_dim, num_heads, num_groups)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim, elementwise_affine=False, eps=1e-6)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, embed_dim)\n",
        "        )\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(embed_dim, 6 * embed_dim, bias=True)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=2)\n",
        "        x = x + gate_msa * self.attention(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        x = self.norm1(x)\n",
        "        x = x + gate_mlp * self.feedforward(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "class GeneticDiffusionModuleBlock(nn.Module):\n",
        "    def __init__(self, channels: int, num_diffusion_steps: int = 100, training: bool = False, depth: int = 3):\n",
        "        super(GeneticDiffusionModuleBlock, self).__init__()\n",
        "        self.channels = channels\n",
        "        assert channels % 8 == 0, \"channels must be divisible by 64\"\n",
        "        num_heads = channels // 8\n",
        "        self.num_diffusion_steps = num_diffusion_steps\n",
        "        self.time_embeddings = nn.Parameter(torch.randn(num_diffusion_steps, context_size(), channels))\n",
        "        self.training = training\n",
        "        self.depth = depth\n",
        "        self.noise_scale = nn.Parameter(torch.linspace(1.0, 0.01, num_diffusion_steps))\n",
        "\n",
        "        # Custom transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(embed_dim=channels, num_heads=num_heads, num_groups=num_heads//2, feedforward_dim=channels * 4) for _ in range(3)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: Tensor = None, ground_truth: Tensor = None):\n",
        "        batch_size, num_nodes, k = x.size()\n",
        "        x_1 = x.clone()\n",
        "\n",
        "        # Simulate the multi-step diffusion process\n",
        "        for step in range(self.num_diffusion_steps):\n",
        "            noise_level = self.noise_scale[step]  # Get the noise level for the current step\n",
        "            noise = torch.randn_like(x) * noise_level  # Generate noise scaled by the noise level\n",
        "            x_1 = x_1 + noise  # Add noise to the input\n",
        "\n",
        "            c = self.time_embeddings[step] # Get the time embedding for the current step\n",
        "            c = c.unsqueeze(0).repeat(batch_size, 1, 1)  # Shape: [batch_size, 1000, channels]\n",
        "\n",
        "            # Apply custom transformer layers\n",
        "            for transformer_layer in self.transformer_layers:\n",
        "                x_1 = transformer_layer(x_1, c)\n",
        "\n",
        "\n",
        "        if self.training and ground_truth is not None:\n",
        "            loss = F.mse_loss(x_1, ground_truth)\n",
        "            return x_1, loss\n",
        "\n",
        "        return x_1\n",
        "\n",
        "class GeneticDiffusion(nn.Module):\n",
        "    def __init__(self, channels: int, num_diffusion_steps: int = 10, k: int = 64, embeddings: int = 384, training: bool = False, depth: int = 3):\n",
        "        super(GeneticDiffusion, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.num_diffusion_steps = num_diffusion_steps\n",
        "        self.training = training\n",
        "        self.depth = depth\n",
        "        self.convlayers = nn.Conv1d(k*embeddings, channels, 1)\n",
        "        # Layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            GeneticDiffusionModuleBlock(channels, num_diffusion_steps, training, depth) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: Tensor = None, ground_truth: Tensor = None):\n",
        "        # Assuming input x shape is [batch_size, nodes, k, embeddings]\n",
        "        batch_size, nodes, k, embeddings = x.size()\n",
        "        # Flatten the k neighbors' embeddings per node into a 1D vector\n",
        "        x = x.view(batch_size, nodes, -1)  # [batch_size, nodes, k * embeddings]\n",
        "        # Pad nodes to ensure a total of context_size nodes per batch\n",
        "        padding = torch.zeros(batch_size, context_size() - nodes, k * embeddings).to(x.device)\n",
        "        if nodes < context_size():\n",
        "            x = torch.cat([x, padding], dim=1)  # [batch_size, context_size, k * embeddings]\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, k * embeddings, context_size]\n",
        "        x = self.convlayers(x)  # [batch_size, channels, context_size]\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, context_size, channels]\n",
        "        # Apply GeneticDiffusionModuleBlock\n",
        "        loss = None\n",
        "        if self.training and ground_truth is not None:\n",
        "            ground_truth = ground_truth.view(batch_size, nodes, -1)\n",
        "            if nodes<context_size():\n",
        "              ground_truth = torch.cat([ground_truth, padding], dim=1)\n",
        "            ground_truth = ground_truth.permute(0, 2, 1)\n",
        "            ground_truth = self.convlayers(ground_truth)\n",
        "            ground_truth = ground_truth.permute(0, 2, 1)\n",
        "            for layer in self.layers:\n",
        "                x, loss = layer(x, ground_truth)\n",
        "            return x, loss\n",
        "        else:\n",
        "            for layer in self.layers:\n",
        "                x = layer(x)\n",
        "            return x\n",
        "\n",
        "# # Example usage with fixed number of nodes per batch\n",
        "# batch_size = 10\n",
        "# nodes = 50  # Fixed number of nodes\n",
        "# k = 64\n",
        "# embeddings = 384\n",
        "# channels = 128  # Number of channels for the diffusion\n",
        "\n",
        "# # Creating a dataset with a fixed number of nodes\n",
        "# dummy_inputs = [torch.randn(batch_size, nodes, k, embeddings)]\n",
        "# dummy_ground_truths = [torch.randn(batch_size, nodes, k, embeddings)]\n",
        "# num_diffusion_steps = 100 #@param\n",
        "# model = GeneticDiffusion(channels=channels, num_diffusion_steps=num_diffusion_steps, training=True, depth=3)\n",
        "\n",
        "# for dummy_input, dummy_ground_truth in zip(dummy_inputs, dummy_ground_truths):\n",
        "#     output, loss = model(dummy_input, dummy_ground_truth)\n",
        "#     print(f\"Output shape: {output.shape}\")\n",
        "#     print(f\"Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trying with Conv of nodes also\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "#@markdown Need to figure out best context scale size (must be a factor od 2000)\n",
        "context_scale = 10 #@param\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_groups):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = int(num_heads)  # Ensure num_heads is an integer\n",
        "        self.num_groups = num_groups\n",
        "        self.head_dim = embed_dim // self.num_heads  # Ensure head_dim is an integer\n",
        "\n",
        "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "        assert embed_dim == self.embed_dim, \"Input embedding dimension must match model embedding dimension\"\n",
        "\n",
        "        qkv = self.qkv_proj(x)\n",
        "        qkv = qkv.view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_len, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        assert seq_len % self.num_groups == 0, \"seq_len must be divisible by num_groups\"\n",
        "        group_size = seq_len // self.num_groups\n",
        "\n",
        "        # Group queries\n",
        "        q_groups = q.view(batch_size, self.num_heads, self.num_groups, group_size, self.head_dim)  # (batch_size, num_heads, num_groups, group_size, head_dim)\n",
        "        k_groups = k.view(batch_size, self.num_heads, self.num_groups, group_size, self.head_dim)\n",
        "        v_groups = v.view(batch_size, self.num_heads, self.num_groups, group_size, self.head_dim)\n",
        "\n",
        "        # Compute attention for each group\n",
        "        attn_scores = torch.einsum('bhgqd,bhgkd->bhgqk', q_groups, k_groups)  # (batch_size, num_heads, num_groups, group_size, group_size)\n",
        "        attn_scores = attn_scores / (self.head_dim ** 0.5)\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        attn_output = torch.einsum('bhgqk,bhgvd->bhgqd', attn_probs, v_groups)  # (batch_size, num_heads, num_groups, group_size, head_dim)\n",
        "        attn_output = attn_output.contiguous().view(batch_size, self.num_heads, seq_len, self.head_dim)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        return self.out_proj(attn_output)\n",
        "\n",
        "def modulate(normed_x, shift, scale):\n",
        "    return normed_x * (1 + scale) + shift\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_groups, feedforward_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim, elementwise_affine=False, eps=1e-6)\n",
        "        self.attention = GroupedQueryAttention(embed_dim, num_heads, num_groups)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim, elementwise_affine=False, eps=1e-6)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, embed_dim)\n",
        "        )\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(embed_dim, 6 * embed_dim, bias=True)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=2)\n",
        "        x = x + gate_msa * self.attention(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        x = self.norm1(x)\n",
        "        x = x + gate_mlp * self.feedforward(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "class GeneticDiffusionModuleBlock(nn.Module):\n",
        "    def __init__(self, channels: int, num_diffusion_steps: int = 100, training: bool = False, depth: int = 3):\n",
        "        super(GeneticDiffusionModuleBlock, self).__init__()\n",
        "        self.channels = channels\n",
        "        assert channels % 8 == 0, \"channels must be divisible by 64\"\n",
        "        num_heads = channels // 8\n",
        "        self.num_diffusion_steps = num_diffusion_steps\n",
        "        self.time_embeddings = nn.Parameter(torch.randn(num_diffusion_steps, 2000//context_scale, channels))\n",
        "        self.training = training\n",
        "        self.depth = depth\n",
        "        self.noise_scale = nn.Parameter(torch.linspace(1.0, 0.01, num_diffusion_steps))\n",
        "\n",
        "        # Custom transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(embed_dim=channels, num_heads=num_heads, num_groups=num_heads//2, feedforward_dim=channels * 4) for _ in range(3)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: Tensor = None, ground_truth: Tensor = None):\n",
        "        batch_size, num_nodes, k = x.size()\n",
        "        x_1 = x.clone()\n",
        "\n",
        "        # Simulate the multi-step diffusion process\n",
        "        for step in range(self.num_diffusion_steps):\n",
        "            noise_level = self.noise_scale[step]  # Get the noise level for the current step\n",
        "            noise = torch.randn_like(x) * noise_level  # Generate noise scaled by the noise level\n",
        "            x_1 = x_1 + noise  # Add noise to the input\n",
        "\n",
        "            c = self.time_embeddings[step] # Get the time embedding for the current step\n",
        "            c = c.unsqueeze(0).repeat(batch_size, 1, 1)  # Shape: [batch_size, 1000, channels]\n",
        "\n",
        "            # Apply custom transformer layers\n",
        "            for transformer_layer in self.transformer_layers:\n",
        "                x_1 = transformer_layer(x_1, c)\n",
        "\n",
        "\n",
        "        if self.training and ground_truth is not None:\n",
        "            loss = F.mse_loss(x_1, ground_truth)\n",
        "            return x_1, loss\n",
        "\n",
        "        return x_1\n",
        "\n",
        "class GeneticDiffusion(nn.Module):\n",
        "    def __init__(self, channels: int, num_diffusion_steps: int = 10, k: int = 64, embeddings: int = 384, training: bool = False, depth: int = 3):\n",
        "        super(GeneticDiffusion, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.num_diffusion_steps = num_diffusion_steps\n",
        "        self.training = training\n",
        "        self.depth = depth\n",
        "\n",
        "        # 2D convolutional layer to transform the input tensor from [10, 2000, 24576] to [10, 200, 128]\n",
        "        self.conv = nn.Conv2d(in_channels=1,\n",
        "                                out_channels=1,\n",
        "                                kernel_size=(context_scale, 192),\n",
        "                                stride=(context_scale, 192),\n",
        "                                padding=0)\n",
        "        # Layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            GeneticDiffusionModuleBlock(channels, num_diffusion_steps, training, depth) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: Tensor = None, ground_truth: Tensor = None):\n",
        "        # Assuming input x shape is [batch_size, nodes, k, embeddings]\n",
        "        batch_size, nodes, k, embeddings = x.size()\n",
        "        # Flatten the k neighbors' embeddings per node into a 1D vector\n",
        "        x = x.view(batch_size, nodes, -1)  # [batch_size, nodes, k * embeddings]\n",
        "        # Pad nodes to ensure a total of context_size nodes per batch\n",
        "        padding = torch.zeros(batch_size, 2000 - nodes, k * embeddings).to(x.device)\n",
        "        if nodes < 2000:\n",
        "            x = torch.cat([x, padding], dim=1)  # [batch_size, context_size, k * embeddings]\n",
        "            if self.training and ground_truth is not None:\n",
        "              ground_truth = ground_truth.view(batch_size, nodes, -1)\n",
        "              ground_truth = torch.cat([ground_truth, padding], dim=1)\n",
        "              ground_truth = ground_truth.unsqueeze(1)  # Add channel dimension\n",
        "              ground_truth = self.conv(ground_truth)\n",
        "              # Reshape back to desired shape: [batch_size, height, width]\n",
        "              ground_truth = ground_truth.squeeze(1)\n",
        "\n",
        "\n",
        "        # Reshape to fit Conv2d input: [batch_size, channels, height, width]\n",
        "        x = x.unsqueeze(1)  # Add channel dimension\n",
        "        x = self.conv(x)\n",
        "        # Reshape back to desired shape: [batch_size, height, width]\n",
        "        x = x.squeeze(1)  # Remove channel dimension\n",
        "\n",
        "        # Apply GeneticDiffusionModuleBlock\n",
        "        loss = None\n",
        "        if self.training and ground_truth is not None:\n",
        "            for layer in self.layers:\n",
        "                x, loss = layer(x, ground_truth)\n",
        "            return x, loss\n",
        "        else:\n",
        "            for layer in self.layers:\n",
        "                x = layer(x)\n",
        "            return x\n",
        "\n",
        "# Example usage with fixed number of nodes per batch\n",
        "batch_size = 10\n",
        "nodes = 1929  # Fixed number of nodes\n",
        "k = 64\n",
        "embeddings = 384\n",
        "channels = 128  # Number of channels for the diffusion\n",
        "\n",
        "# Creating a dataset with a fixed number of nodes\n",
        "dummy_inputs = [torch.randn(batch_size, nodes, k, embeddings)]\n",
        "dummy_ground_truths = [torch.randn(batch_size, nodes, k, embeddings)]\n",
        "num_diffusion_steps = 100 #@param\n",
        "model = GeneticDiffusion(channels=channels, num_diffusion_steps=num_diffusion_steps, training=True, depth=3)\n",
        "\n",
        "for dummy_input, dummy_ground_truth in zip(dummy_inputs, dummy_ground_truths):\n",
        "    output, loss = model(dummy_input, dummy_ground_truth)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU3w5gXfplj5",
        "outputId": "5cf71790-a6aa-4405-b051-0ae2a6a60938"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([10, 200, 128])\n",
            "Loss: 1.3380261659622192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This diffuison model with convulution of the nodes was able to run on about 100 gigs with the batch size that I have currently of 10"
      ],
      "metadata": {
        "id": "DapEgwIbxATy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Module from with BitNet implementation\n",
        "\n",
        "Below are diffusion modules to test BitNet implemenation"
      ],
      "metadata": {
        "id": "Z_s4nhePIFDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown - Need to finalize padding size\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "pip install bitnet\n",
        "from bitnet import BitFeedForward, BitMGQA\n",
        "\n",
        "def context_size():\n",
        "  x = 2000 #@param\n",
        "  return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer module that applies multi-head attention and feed-forward layers.\n",
        "\n",
        "    Args:\n",
        "        dim (int): The dimension of the input and output tensors.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The number of transformer layers.\n",
        "        ff_mult (int, optional): The multiplier for the hidden dimension in the feed-forward layers.\n",
        "            Defaults to 2.\n",
        "        *args: Variable length argument list.\n",
        "        **kwargs: Arbitrary keyword arguments.\n",
        "\n",
        "    Attributes:\n",
        "        layers (nn.ModuleList): List of multi-head attention layers.\n",
        "        ffn_layers (nn.ModuleList): List of feed-forward layers.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dim: int, heads: int, depth: int, ff_mult: int = 2, *args, **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.ffn_layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(BitMGQA(dim, heads, *args, **kwargs))\n",
        "\n",
        "            self.ffn_layers.append(\n",
        "                BitFeedForward(\n",
        "                    dim,\n",
        "                    dim,\n",
        "                    ff_mult,\n",
        "                    swish=True,\n",
        "                    post_act_ln=True,\n",
        "                    dropout=0.1,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor, *args, **kwargs) -> Tensor:\n",
        "        skip = x\n",
        "        for attn, ffn in zip(self.layers, self.ffn_layers):\n",
        "            x, _ = attn(x, x, x, is_causal=True, *args, **kwargs)\n",
        "            x = x + skip\n",
        "            x = ffn(x) + x\n",
        "        return x\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, channels: int, num_diffusion_steps: int = 10, k: int = 64, embeddings: int = 384, training: bool = False, depth: int = 3):\n",
        "        super(Diffusion, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.num_diffusion_steps = num_diffusion_steps\n",
        "        self.training = training\n",
        "        self.depth = depth\n",
        "        self.conv = nn.Conv1d(k*embeddings, channels, 1)\n",
        "        # Layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            GeneticDiffusionModuleBlock(channels, num_diffusion_steps, training, depth) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: Tensor = None, ground_truth: Tensor = None):\n",
        "        # Assuming input x shape is [batch_size, nodes, k, embeddings]\n",
        "        batch_size, nodes, k, embeddings = x.size()\n",
        "        # Flatten the k neighbors' embeddings per node into a 1D vector\n",
        "        x = x.view(batch_size, nodes, -1)  # [batch_size, nodes, k * embeddings]\n",
        "        # Pad nodes to ensure a total of context_size nodes per batch\n",
        "        padding = torch.zeros(batch_size, context_size() - nodes, k * embeddings).to(x.device)\n",
        "        if nodes < context_size():\n",
        "            x = torch.cat([x, padding], dim=1)  # [batch_size, context_size, k * embeddings]\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, k * embeddings, context_size]\n",
        "        x = self.conv(x)  # [batch_size, channels, context_size]\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, context_size, channels]\n",
        "        # Apply GeneticDiffusionModuleBlock\n",
        "        loss = None\n",
        "        if self.training and ground_truth is not None:\n",
        "            ground_truth = ground_truth.view(batch_size, nodes, -1)\n",
        "            if nodes<context_size():\n",
        "              ground_truth = torch.cat([ground_truth, padding], dim=1)\n",
        "            ground_truth = ground_truth.permute(0, 2, 1)\n",
        "            ground_truth = self.convlayers(ground_truth)\n",
        "            ground_truth = ground_truth.permute(0, 2, 1)\n",
        "            for layer in self.layers:\n",
        "                x, loss = layer(x, ground_truth)\n",
        "            return x, loss\n",
        "        else:\n",
        "            for layer in self.layers:\n",
        "                x = layer(x)\n",
        "            return x"
      ],
      "metadata": {
        "id": "goNanlhkMwDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters to Test"
      ],
      "metadata": {
        "id": "NfOeNuMUfFFZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jtivtswsHu68"
      },
      "outputs": [],
      "source": [
        "channels = 128 #@param\n",
        "num_diffusion_steps = 100 #@param\n",
        "depth = 3 #@param\n",
        "learnRate=1e-3 #@param\n",
        "\n",
        "model = GeneticDiffusion(channels=channels,num_diffusion_steps=num_diffusion_steps,training=True, depth=depth)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learnRate)\n",
        "epochs = 3 #@param\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH0QMtaAHu68"
      },
      "source": [
        "# Training loop for diffusion model\n",
        "- create loop for training diffusion model\n",
        "- ouput trained weights to be used for further training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zM2emdSkme6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5abb9884-9fd0-4fc6-f210-12ffdf686b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: torch.Size([9, 1079, 64, 384])\n",
            "StepAhead shape: torch.Size([9, 1079, 64, 384])\n",
            "Loss: 1.0224237442016602\n",
            "Loss: 1.018386721611023\n",
            "Loss: 1.0030959844589233\n",
            "Data shape: torch.Size([9, 1079, 64, 384])\n",
            "StepAhead shape: torch.Size([9, 1079, 64, 384])\n",
            "Loss: 1.0045024156570435\n",
            "Loss: 0.9985254406929016\n",
            "Loss: 0.9880445599555969\n",
            "Data shape: torch.Size([9, 1079, 64, 384])\n",
            "StepAhead shape: torch.Size([9, 1079, 64, 384])\n",
            "Loss: 0.9712555408477783\n",
            "Loss: 0.9461786150932312\n",
            "Loss: 0.9121125340461731\n",
            "Data shape: torch.Size([9, 1079, 64, 384])\n",
            "StepAhead shape: torch.Size([9, 1079, 64, 384])\n",
            "Loss: 0.8726099133491516\n",
            "Loss: 0.8312634229660034\n",
            "Loss: 0.7910821437835693\n"
          ]
        }
      ],
      "source": [
        "for data, stepAhead in data_loader:    # Process your data\n",
        "    # data will be a batch of your tensors\n",
        "    # stepAhead will be the ground truth tensor\n",
        "    # Train your model\n",
        "    data = data[0]\n",
        "    stepAhead = stepAhead[0]\n",
        "    print(f\"Data shape: {data.shape}\")\n",
        "    print(f\"StepAhead shape: {stepAhead.shape}\")\n",
        "    for _ in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        y, loss = model(data, stepAhead)\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}